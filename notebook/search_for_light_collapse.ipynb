{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "import torch \n",
    "from PIL import Image \n",
    "\n",
    "import os \n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "font = ImageFont.truetype(\"/usr/share/fonts/truetype/noto/NotoSansMono-Regular.ttf\", 32)\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import skimage\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECTS = [\n",
    "    'shoe',\n",
    "]\n",
    "SCALES = [-1.0, 1.0]\n",
    "LORA_CHKPT = range(100, 40000, 100)\n",
    "#SEEDS = [807, 200, 201, 202, 800]\n",
    "SEEDS = range(0,10000, 100)\n",
    "LEARNING_RATE = \"5e-5\"\n",
    "#ROOT_DIR = \"../output/chkpt100/512_unsplash250_cast_learning_rate/\"\n",
    "ROOT_DIR = \"../output/chkpt100/512_unsplash250_cast_learning_rate/ckpt10000_5e-5_bottle_minus1\"\n",
    "# COLOR_TYPE=\"gray\"\n",
    "# FOLDER_OUT = f\"{ROOT_DIR}/{LEARNING_RATE}/{COLOR_TYPE}_with_text\"\n",
    "# FOLDER_IN = f\"{ROOT_DIR}/{LEARNING_RATE}/{COLOR_TYPE}\"\n",
    "# FOLDER_VID = f\"{ROOT_DIR}/{LEARNING_RATE}/{COLOR_TYPE}_video\"\n",
    "# FOLDER_ROW = f\"{ROOT_DIR}/{LEARNING_RATE}/{COLOR_TYPE}_row\"\n",
    "\n",
    "COLOR_TYPE=\"gray\"\n",
    "FOLDER_OUT = f\"{ROOT_DIR}/{COLOR_TYPE}_with_text\"\n",
    "FOLDER_IN = f\"{ROOT_DIR}/{COLOR_TYPE}\"\n",
    "FOLDER_VID = f\"{ROOT_DIR}/{COLOR_TYPE}_video\"\n",
    "FOLDER_ROW = f\"{ROOT_DIR}/{COLOR_TYPE}_row\"\n",
    "os.makedirs(FOLDER_OUT, exist_ok=True)\n",
    "os.makedirs(FOLDER_VID, exist_ok=True)\n",
    "os.makedirs(FOLDER_ROW, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5462e0f970ff4f1b8866d51625d8abfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0000_0000_0000_0000.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0000_0000_0000_0001.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0000_0000_0000_0002.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0000_0000_0000_0003.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0000_0000_0000_0004.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0000_0000_0001_0000.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0000_0000_0001_0001.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0000_0000_0001_0002.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0000_0000_0001_0003.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0000_0000_0001_0004.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0001_0000_0000_0000.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0001_0000_0000_0001.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0001_0000_0000_0002.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0001_0000_0000_0003.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0001_0000_0000_0004.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0001_0000_0001_0000.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0001_0000_0001_0001.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0001_0000_0001_0002.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0001_0000_0001_0003.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0001_0000_0001_0004.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0002_0000_0000_0000.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0002_0000_0000_0001.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0002_0000_0000_0002.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0002_0000_0000_0003.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0002_0000_0000_0004.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0002_0000_0001_0000.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0002_0000_0001_0001.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0002_0000_0001_0002.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0002_0000_0001_0003.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0002_0000_0001_0004.png\n",
      "../output/chkpt100/512_unsplash250_cast_learning_rate//5e-5/gray/0003_0000_0000_0000.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m fpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFOLDER_IN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlora_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(fpath)\n\u001b[0;32m----> 7\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m draw \u001b[38;5;241m=\u001b[39m ImageDraw\u001b[38;5;241m.\u001b[39mDraw(image)\n\u001b[1;32m      9\u001b[0m draw\u001b[38;5;241m.\u001b[39mtext((\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m),\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlora_name\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mprompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mscale: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale_name\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mseed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed_name\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLEARNING_RATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,(\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m),font\u001b[38;5;241m=\u001b[39mfont)\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/PIL/Image.py:3140\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3137\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m   3138\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3140\u001b[0m prefix \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m   3142\u001b[0m preinit()\n\u001b[1;32m   3144\u001b[0m accept_warnings \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for lora_id, lora_name in enumerate(tqdm(LORA_CHKPT)):\n",
    "    for prompt_id, prompt_name in enumerate(OBJECTS):\n",
    "        for scale_id, scale_name in enumerate(SCALES):\n",
    "            for seed_id, seed_name in enumerate(SEEDS):\n",
    "                fpath = f\"{FOLDER_IN}/{lora_id:04d}_{prompt_id:04d}_{scale_id:04d}_{seed_id:04d}.png\"\n",
    "                #print(fpath)\n",
    "                image = Image.open(fpath)\n",
    "                draw = ImageDraw.Draw(image)\n",
    "                draw.text((10, 10),f\"step: {lora_name:5d}\\nprompt: {prompt_name}\\nscale: {scale_name:.2f}\\nseed: {seed_name:3d}\\nLR: {LEARNING_RATE}\",(255,255,255),font=font)\n",
    "                image.save(f\"{FOLDER_OUT}/{lora_id:04d}_{prompt_id:04d}_{scale_id:04d}_{seed_id:04d}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e7006aba4f4abfac7c39c8f9f9a1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for lora_id, lora_name in enumerate(tqdm(LORA_CHKPT)):\n",
    "    for scale_id, scale_name in enumerate(SCALES):\n",
    "        images = []\n",
    "        for prompt_id, prompt_name in enumerate(OBJECTS):\n",
    "            for seed_id, seed_name in enumerate(SEEDS):\n",
    "                fpath = os.path.join(FOLDER_OUT, f'{lora_id:04d}_{prompt_id:04d}_{scale_id:04d}_{seed_id:04d}.png')\n",
    "                image = torchvision.io.read_image(fpath)\n",
    "                image = torchvision.transforms.functional.resize(image, (256,256))\n",
    "                images.append(image)\n",
    "        images = torch.stack(images)\n",
    "        #grid = torchvision.utils.make_grid(images, nrow=len(SEEDS))\n",
    "        grid = torchvision.utils.make_grid(images, nrow=100)\n",
    "        grid = torchvision.transforms.ToPILImage()(grid)\n",
    "        output_name = f\"{FOLDER_ROW}/{lora_id:04d}_{scale_id:04d}.png\"\n",
    "        grid.save(output_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, image2, from '../output/chkpt100/512_unsplash250_learning_rate/1e-3/gray_row/%04d_0000.png':\n",
      "  Duration: 00:00:00.80, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgb24(pc), 1292x260, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "File '../output/chkpt100/512_unsplash250_learning_rate/1e-3/gray_video/0000.mp4' already exists. Overwrite? [y/N] "
     ]
    }
   ],
   "source": [
    "#FOLDER_VID = f\"../output/chkpt100/unsplash250_sh1/gray_row2000\"\n",
    "#FOLDER_ROW = f\"../output/chkpt100/unsplash250_sh1/gray_video2000\"\n",
    "#os.makedirs(FOLDER_VID, exist_ok=True)\n",
    "#os.makedirs(FOLDER_ROW, exist_ok=True)\n",
    "SCALE_ID = [0, 1]\n",
    "for scale_id in (SCALE_ID):\n",
    "    cmd = f'ffmpeg -r 5 -i \"{FOLDER_ROW}/%04d_{scale_id:04d}.png\" -c:v libx264 -crf 12 -pix_fmt yuv420p -vf pad=\"width=ceil(iw/2)*2:height=ceil(ih/2)*2\" {FOLDER_VID}/{scale_id:04d}.mp4'\n",
    "    os.system(cmd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
